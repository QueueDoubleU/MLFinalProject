{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO1mRyJrTdRMIJWUcPotoKO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["We used 3 different data sources and the transcripts took up a lot of memory. Since we experimented with the data at different levels, the following steps outline how we loaded and cleaned our data:\n","\n","**Cornell Data**\n","* Create folder with transcripts at conversation level (entire case) \n","* Create a folder with transcripts at the utterance level (a few sentences)\n","* Create dataframe of case information at conversation level\n","* Create a dataframe of case information at utterance level\n","\n","Since we work with the data in different ways, we only add the transcripts to our dataframe right before we use them in our models. \n","\n","**Martin Quinn Data**\n","* Load in Martin Quinn scores and add to both transcript and utterance dataframes\n","\n","**Washington University**\n","* Load in additional attribute data to transcript dataframes"],"metadata":{"id":"1fCxloWacBgr"}},{"cell_type":"markdown","source":["## Cornell Data\n","\n"],"metadata":{"id":"C-rRO7AGIz3d"}},{"cell_type":"markdown","source":["Cornell collected supreme court transcripts and built a package to load them. We will use this to build our dataframe."],"metadata":{"id":"c37dijLUJIpp"}},{"cell_type":"code","source":["#install 'convokit' which is Cornell's supreme court python package\n","!pip3 install convokit\n","!python3 -m spacy download en_core_web_sm\n","\n","#mount our drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","#load packages\n","import pandas as pd\n","import numpy as np\n","from convokit import Corpus, download"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bt2WJHH_JMcl","executionInfo":{"status":"ok","timestamp":1683300739882,"user_tz":240,"elapsed":81105,"user":{"displayName":"Kenan Carames","userId":"14279214868438344941"}},"outputId":"84e87155-8b3d-41a1-e567-13eed7f16c82"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting convokit\n","  Downloading convokit-2.5.3.tar.gz (167 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.0/168.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n","Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.5.3)\n","Collecting msgpack-numpy>=0.4.3.2\n","  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n","Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.5.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.10.1)\n","Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.2)\n","Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n","Collecting dill>=0.2.9\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.0)\n","Collecting clean-text>=0.1.1\n","  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n","Collecting unidecode>=1.1.1\n","  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting emoji<2.0.0,>=1.0.0\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy<7.0,>=6.0\n","  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (8.4.0)\n","Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.22.4)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.0.9)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.11.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (23.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.39.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.4)\n","Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.5)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2022.10.31)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (4.65.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2022.7.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.1.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.10.1)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.1.9)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.4)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.7.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.9)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.6)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.10.7)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (6.3.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.27.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (67.7.2)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.1)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.1.1->convokit) (0.2.6)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=2.3.5->convokit) (4.5.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2022.12.7)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.0.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.2)\n","Building wheels for collected packages: convokit, emoji\n","  Building wheel for convokit (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for convokit: filename=convokit-2.5.3-py3-none-any.whl size=204125 sha256=2d2b3311cbba96b8cba758f87f905034d325deb0dcfac6e574db94718e122e5a\n","  Stored in directory: /root/.cache/pip/wheels/8f/da/dd/d65869bf6766b536f422e0a9753e9cf98bb9df7904b5b9c4a5\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171048 sha256=0cf47f02d4e37439943865517c644376c86b2faa50085a5874fa093dd599cb86\n","  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n","Successfully built convokit emoji\n","Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dill, clean-text, convokit\n","Successfully installed clean-text-0.6.0 convokit-2.5.3 dill-0.3.6 emoji-1.7.0 ftfy-6.1.1 msgpack-numpy-0.4.8 unidecode-1.3.6\n","2023-05-05 15:31:28.793042: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-05-05 15:31:29.986273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting en-core-web-sm==3.5.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n","Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.15)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["corpus = Corpus(filename=download(\"supreme-corpus\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5htXj4gMKnzN","executionInfo":{"status":"ok","timestamp":1683300969739,"user_tz":240,"elapsed":229863,"user":{"displayName":"Kenan Carames","userId":"14279214868438344941"}},"outputId":"ff5cf202-e6b5-4c1d-d208-d725a74cb7d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading supreme-corpus to /root/.convokit/downloads/supreme-corpus\n","Downloading supreme-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/supreme-corpus/supreme-corpus.zip (1255.8MB)... Done\n"]}]},{"cell_type":"markdown","source":["Now that we have the corpus of cases, we will get them at the CONVERSATION level. Each conversation has a unique transcript so it will be easy to match with our transcripts later. However, since the transcripts take up so much storage, we have downloaded those in a folder on our Google Drive and will only call those into our dataframe when building our DNNs later. The code to write those files are below:"],"metadata":{"id":"5JQuh5qZLiPP"}},{"cell_type":"code","source":["#dataframe for conversations\n","df = corpus.get_conversations_dataframe()\n"],"metadata":{"id":"arum1mnjKqy9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# add convo_id as column rather than index\n","df['convo_id'] = df.index\n","#remove index\n","df = df.reset_index().drop(['id'], axis=1)"],"metadata":{"id":"ZZpHlFkOOD7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#function to get transcript, given a convo_id\n","def get_transcript(convo_id, remove_return=False):\n","\n","  # pull conversation object\n","  convo = corpus.get_conversation(convo_id)\n","\n","  # from conversation object, create list of utterance (i.e. the text bits) ids\n","  convo_utts = list(convo.iter_utterances())\n","  \n","  # combine text data from all utterances \n","  if remove_return:\n","    convo_transcript = [utt.text.replace('\\n', ' ') for utt in convo_utts]\n","  else:\n","    convo_transcript = [utt.text for utt in convo_utts]\n","\n","  # join elements of list \n","  convo_transcript = ''.join(convo_transcript)\n","  \n","  return convo_transcript\n","\n","\n","#make empty case_transcript column\n","df['case_transcript'] = np.nan\n","\n","# populate df with transcripts\n","for i, id in enumerate(df.convo_id):\n","\n","  transcript = get_transcript(id)\n","\n","  try:\n","    df.at[i, 'case_transcript'] = transcript\n","\n","  except: \n","    pass\n","\n","\n","# write transcripts to files\n","for i, transcript in enumerate(df.case_transcript):\n","  # set file name\n","  file_name = str(df.iloc[i]['meta.case_id'])\n","  # set path name\n","  path = f'/content/drive/MyDrive/INFO251Final/Transcripts_Case_Convo/{file_name}.txt'\n","  # get transcript for case\n","  transcript = df.iloc[i]['case_transcript']\n","\n","  with open(path, 'w') as convo_transcript: \n","    convo_transcript.write(transcript)"],"metadata":{"id":"eg5RdShcMEWj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv('/content/drive/MyDrive/INFO251Final/ArgumentsTable.csv')"],"metadata":{"id":"yrKHqb7fO3YK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Since the Arguments table was so much storage, we ended up using the dataframes without the transcripts included and then added them right before modeling. "],"metadata":{"id":"bUo9tl0Dil8D"}},{"cell_type":"markdown","source":["Get dataframes of conversations and utterances WITHOUT actual transcripts attached"],"metadata":{"id":"VsKgC9lmh_s8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6_eyxOhqCIsz","colab":{"base_uri":"https://localhost:8080/","height":673},"executionInfo":{"status":"ok","timestamp":1683302001020,"user_tz":240,"elapsed":1821,"user":{"displayName":"Kenan Carames","userId":"14279214868438344941"}},"outputId":"97194f3a-e1d6-4861-c1b9-df14fdce1d7c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      vectors  meta.case_id  \\\n","id                            \n","13127      []       1955_71   \n","12997      []      1955_410   \n","13024      []      1955_410   \n","13015      []      1955_351   \n","13016      []       1955_38   \n","...       ...           ...   \n","24998      []   2019_19-635   \n","24978      []    2019_19-46   \n","24979      []   2019_19-177   \n","24972      []  2019_18-1584   \n","24969      []    2019_19-67   \n","\n","                                          meta.advocates meta.win_side  \\\n","id                                                                       \n","13127  {'harry_f_murphy': {'side': 1, 'role': 'inferr...             0   \n","12997  {'howard_c_westwood': {'side': 1, 'role': 'inf...             1   \n","13024  {'howard_c_westwood': {'side': 1, 'role': 'inf...             1   \n","13015  {'harry_d_graham': {'side': 3, 'role': 'inferr...             1   \n","13016  {'robert_n_gorman': {'side': 3, 'role': 'infer...             0   \n","...                                                  ...           ...   \n","24998  {'jay_alan_sekulow': {'side': 1, 'role': 'for ...             0   \n","24978  {'erica_l_ross': {'side': 1, 'role': 'Assistan...             0   \n","24979  {'christopher_g_michel': {'side': 1, 'role': '...             1   \n","24972  {'anthony_a_yang': {'side': 1, 'role': 'for th...             1   \n","24969  {'eric_j_feigin': {'side': 1, 'role': 'for the...             1   \n","\n","                                         meta.votes_side  \n","id                                                        \n","13127  {'j__john_m_harlan2': 0, 'j__hugo_l_black': 0,...  \n","12997  {'j__john_m_harlan2': 1, 'j__hugo_l_black': 1,...  \n","13024  {'j__john_m_harlan2': 1, 'j__hugo_l_black': 1,...  \n","13015  {'j__john_m_harlan2': 1, 'j__hugo_l_black': 1,...  \n","13016  {'j__john_m_harlan2': 0, 'j__hugo_l_black': 0,...  \n","...                                                  ...  \n","24998  {'j__john_g_roberts_jr': 0, 'j__clarence_thoma...  \n","24978  {'j__john_g_roberts_jr': 0, 'j__clarence_thoma...  \n","24979  {'j__john_g_roberts_jr': 1, 'j__clarence_thoma...  \n","24972  {'j__john_g_roberts_jr': 1, 'j__clarence_thoma...  \n","24969  {'j__john_g_roberts_jr': 1, 'j__clarence_thoma...  \n","\n","[7817 rows x 5 columns]"],"text/html":["\n","  <div id=\"df-1cb89d1f-3233-40c0-85cb-dc1081f178a0\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>vectors</th>\n","      <th>meta.case_id</th>\n","      <th>meta.advocates</th>\n","      <th>meta.win_side</th>\n","      <th>meta.votes_side</th>\n","    </tr>\n","    <tr>\n","      <th>id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>13127</th>\n","      <td>[]</td>\n","      <td>1955_71</td>\n","      <td>{'harry_f_murphy': {'side': 1, 'role': 'inferr...</td>\n","      <td>0</td>\n","      <td>{'j__john_m_harlan2': 0, 'j__hugo_l_black': 0,...</td>\n","    </tr>\n","    <tr>\n","      <th>12997</th>\n","      <td>[]</td>\n","      <td>1955_410</td>\n","      <td>{'howard_c_westwood': {'side': 1, 'role': 'inf...</td>\n","      <td>1</td>\n","      <td>{'j__john_m_harlan2': 1, 'j__hugo_l_black': 1,...</td>\n","    </tr>\n","    <tr>\n","      <th>13024</th>\n","      <td>[]</td>\n","      <td>1955_410</td>\n","      <td>{'howard_c_westwood': {'side': 1, 'role': 'inf...</td>\n","      <td>1</td>\n","      <td>{'j__john_m_harlan2': 1, 'j__hugo_l_black': 1,...</td>\n","    </tr>\n","    <tr>\n","      <th>13015</th>\n","      <td>[]</td>\n","      <td>1955_351</td>\n","      <td>{'harry_d_graham': {'side': 3, 'role': 'inferr...</td>\n","      <td>1</td>\n","      <td>{'j__john_m_harlan2': 1, 'j__hugo_l_black': 1,...</td>\n","    </tr>\n","    <tr>\n","      <th>13016</th>\n","      <td>[]</td>\n","      <td>1955_38</td>\n","      <td>{'robert_n_gorman': {'side': 3, 'role': 'infer...</td>\n","      <td>0</td>\n","      <td>{'j__john_m_harlan2': 0, 'j__hugo_l_black': 0,...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>[]</td>\n","      <td>2019_19-635</td>\n","      <td>{'jay_alan_sekulow': {'side': 1, 'role': 'for ...</td>\n","      <td>0</td>\n","      <td>{'j__john_g_roberts_jr': 0, 'j__clarence_thoma...</td>\n","    </tr>\n","    <tr>\n","      <th>24978</th>\n","      <td>[]</td>\n","      <td>2019_19-46</td>\n","      <td>{'erica_l_ross': {'side': 1, 'role': 'Assistan...</td>\n","      <td>0</td>\n","      <td>{'j__john_g_roberts_jr': 0, 'j__clarence_thoma...</td>\n","    </tr>\n","    <tr>\n","      <th>24979</th>\n","      <td>[]</td>\n","      <td>2019_19-177</td>\n","      <td>{'christopher_g_michel': {'side': 1, 'role': '...</td>\n","      <td>1</td>\n","      <td>{'j__john_g_roberts_jr': 1, 'j__clarence_thoma...</td>\n","    </tr>\n","    <tr>\n","      <th>24972</th>\n","      <td>[]</td>\n","      <td>2019_18-1584</td>\n","      <td>{'anthony_a_yang': {'side': 1, 'role': 'for th...</td>\n","      <td>1</td>\n","      <td>{'j__john_g_roberts_jr': 1, 'j__clarence_thoma...</td>\n","    </tr>\n","    <tr>\n","      <th>24969</th>\n","      <td>[]</td>\n","      <td>2019_19-67</td>\n","      <td>{'eric_j_feigin': {'side': 1, 'role': 'for the...</td>\n","      <td>1</td>\n","      <td>{'j__john_g_roberts_jr': 1, 'j__clarence_thoma...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7817 rows × 5 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1cb89d1f-3233-40c0-85cb-dc1081f178a0')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-1cb89d1f-3233-40c0-85cb-dc1081f178a0 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-1cb89d1f-3233-40c0-85cb-dc1081f178a0');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["#dataframe for conversations\n","df_convo = corpus.get_conversations_dataframe()\n","df_convo"]},{"cell_type":"code","source":["#dataframe for utterances\n","df_utt = corpus.get_utterances_dataframe()\n","df_utt.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"peHfe8lCiNMZ","executionInfo":{"status":"ok","timestamp":1683303086909,"user_tz":240,"elapsed":82598,"user":{"displayName":"Kenan Carames","userId":"14279214868438344941"}},"outputId":"961215a9-e315-4dec-d72a-be46b9a71d61"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['timestamp', 'text', 'speaker', 'reply_to', 'conversation_id',\n","       'meta.case_id', 'meta.start_times', 'meta.stop_times',\n","       'meta.speaker_type', 'meta.side', 'meta.timestamp', 'vectors'],\n","      dtype='object')"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## Martin Quinn Scores"],"metadata":{"id":"8HAPQl9ohyaT"}},{"cell_type":"code","source":["# create datafrae of Martin Quinn scores to merge with dataframes\n","martin_quinn = pd.read_csv('/content/drive/MyDrive/INFO251Final/MartinQuinnScores.csv')"],"metadata":{"id":"Q56bQdtdEP-8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, begin restructuring and merge "],"metadata":{"id":"2bCB2kNmjVLD"}},{"cell_type":"code","source":["# set to run with utterances or conversations \n","utts = False"],"metadata":{"id":"1W6TFIDGht21"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_utts(df_utt):\n","    # updating columns\n","\n","    # rename columns\n","    df_utt = df_utt.rename(columns={'meta.votes_side': 'votes_side',\n","                                      'meta.win_side': 'win_side',\n","                                      'meta.case_id': 'case_id',\n","                                      'med': 'mq_score', \n","                                      'conversation_id': 'convo_id',\n","                                      'term_year': 'term'})\n","\n","    df_utt.term = df_utt.term.astype('int64')\n","\n","    # add MartinQuinn Scores\n","    df_utt = df_utt.merge(martin_quinn[['term', 'med']], on='term')\n","\n","    # drop unused columns\n","    # NOTE we may want to try an analysis on some of these later on\n","    df_utt = df_utt.drop(columns=['speaker', \n","                                  'reply_to', \n","                                  'timestamp', \n","                                  'meta.start_times', \n","                                  'meta.stop_times', \n","                                  'meta.speaker_type',\n","                                  'meta.side',\n","                                  'meta.timestamp',\n","                                  'vectors',\n","                                  'Unnamed: 0'])\n","\n","\n","    # add \"win_side\" to utterance dataframe\n","    df_utt = df_utt.merge(df_convo[['convo_id', 'win_side']], on='convo_id')\n","\n","    #df_utt.drop(columns=['Unnamed: 0'])\n","    df_utt = df_utt.rename(columns={'text': 'words'})\n","\n","    # Remove instances where case outcome was unknown or ????\n","    df_utt.drop(df_utt[df_utt['win_side'] == -1.0].index, inplace = True)\n","    df_utt.drop(df_utt[df_utt['win_side'] == 2.0].index, inplace = True)\n","\n","    # Remove null values from the few cases with incomplete data\n","    df_utt = df_utt.dropna()\n","\n","    return df_utt"],"metadata":{"id":"fFv8gVkBht0u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_utt = clean_utts(df_utt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":336},"id":"IEjRjJnShtyo","executionInfo":{"status":"error","timestamp":1683302802776,"user_tz":240,"elapsed":825,"user":{"displayName":"Kenan Carames","userId":"14279214868438344941"}},"outputId":"da6b8abf-8576-4764-92e2-fd601f7becf5"},"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-92e6e47336b1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_utt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_utts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_utt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-7da2018af0c2>\u001b[0m in \u001b[0;36mclean_utts\u001b[0;34m(df_utt)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                       'term_year': 'term'})\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdf_utt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_utt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# add MartinQuinn Scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'term'"]}]},{"cell_type":"code","source":["#clean unique instance\n","df_utt['mq_score'] = df_utt['mq_score'].str.replace('0.162.5', '0.162')"],"metadata":{"id":"rLTRj9TfqKuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#save utterance dataframe to Drive\n","df_utt.to_csv('/content/drive/MyDrive/INFO251Final/Utterances_Dataframe_CleanedMerged.csv')"],"metadata":{"id":"5zSej0rqnp_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_convos(df_convo):\n","  # Remove instances where case outcome was unknown or ????\n","  df_convo.drop(df_convo[df_convo['win_side'] == -1.0].index, inplace = True)\n","  df_convo.drop(df_convo[df_convo['win_side'] == 2.0].index, inplace = True)\n","\n","  # convert term from object to int\n","  df_convo.term = df_convo.term.astype('int64')\n","\n","  # drop unused columns\n","  # NOTE we may want to try an analysis on some of these later on. Save memory now\n","  df_convo = df_convo.drop(columns=['Unnamed: 0', 'vectors', 'advocates', 'votes_side'])\n","\n","  # 3 cases have null data due to oddities of transcribing data pre-digital transcripts\n","  df_convo = df_convo.dropna()\n","\n","  return df_convo"],"metadata":{"id":"UBUcBlVvhtw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_convo = clean_convos(df_convo)"],"metadata":{"id":"W7zu4v7lhtu2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#clean unique instance\n","df_convo['mq_score'] = df_convo['mq_score'].str.replace('0.162.5', '0.162')"],"metadata":{"id":"qH2UVEzFqd9x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we will merge with Washington data. Since we only used this data for our simple models at the case level, we only need to merge with our df_convo dataframe"],"metadata":{"id":"EFIr2xmOkpge"}},{"cell_type":"code","source":["#load in washington data\n","\n","# error with Wash U data file so we need to find file encoding and add when reading\n","# commented out because now that it is discovered, no need to re-run each time\n","\n","#!pip install chardet\n","\n","#import chardet    \n","#rawdata = open('/content/drive/MyDrive/INFO251Final/WashU_onerowpercaseid.csv', 'rb').read()\n","#result = chardet.detect(rawdata)\n","#charenc = result['encoding']\n","#print(charenc)"],"metadata":{"id":"qTvKKC6pENGZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create dataframe from Washington University data\n","df_wash = pd.read_csv('/content/drive/MyDrive/INFO251Final/WashU_onerowpercaseid.csv', encoding='Windows-1252')"],"metadata":{"id":"S5ahtso5EUOZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Transform some data to words instead of numerical values?\n","\n","# Column: Issue Area\n","# issue area is listed as numerical value but they correspond to different categories\n","# create dictionary of column integers and corresponding meanings\n","data_issueArea = ({'Integer Value':[1,2,3,4,5,6,7,8,9,10,11,12,13,14],\n","                'Issue Area':['Criminal Procedure', 'Civil Rights', 'First Amendment', 'Due Process',\n","                                      'Privacy', 'Attorneys', 'Unions', 'Economic Activity', 'Judicial Power',\n","                                      'Federalism', 'Interstate Relations', 'Federal Taxation', 'Miscellaneous',\n","                                      'Private Action']})\n","# turn into dataframe\n","df_issueArea = pd.DataFrame(data_issueArea)\n","# replace the values in Wash U with the words\n","df_issueArea.set_index('Integer Value', inplace=True)\n","df_wash['issueArea'] = df_wash['issueArea'].map(df_issueArea['Issue Area'])\n","df_issueArea.reset_index(inplace=True)\n","print(df_wash['issueArea'])\n","\n","\n","\n","\n","\n","\n","# Column: lcDispositionDirection\n","# lower court disposition direction is listed as numerical value but they correspond to different categories\n","# create dictionary of column integers and corresponding meanings\n","data_lcdd = ({'Integer Value':[1,2,3],\n","                'direction':['conservative', 'liberal', 'unspecifiable']})\n","\n","# turn into dataframe\n","df_lcdd = pd.DataFrame(data_lcdd)\n","# replace the values in Wash U with the words\n","df_lcdd.set_index('Integer Value', inplace=True)\n","df_wash['lcDispositionDirection'] = df_wash['lcDispositionDirection'].map(df_lcdd['direction'])\n","df_lcdd.reset_index(inplace=True)\n","print(df_wash['lcDispositionDirection'])\n"],"metadata":{"id":"S30ScKoUEv-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#get only important columns\n","df_wash_important = df_wash[['caseId', 'issueArea', 'lcDispositionDirection']]\n","\n","#change case column so it matches df_case\n","df_wash_important = df_wash_important.rename(columns={'caseId':'case_id'})\n","df_wash_important['case_id'] = df_wash_important['case_id'].str.replace('-', '_')\n","\n","df_wash_important.shape"],"metadata":{"id":"N7hbt7ewl1ZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#create additoinal case_id column in df_convo so that we can match in same format as df_wash_important\n","\n","#because later we will need to match transcripts based on original case_id, we create a \n","#duplicate column here to match and then delete extra so transcripts can be matched later\n","df_convo['og_case_id'] = df_convo['case_id']"],"metadata":{"id":"xIE9tedvmOrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#for df_convo, split the case_id column into two parts at the underscore, and add '0' if necessary\n","df_convo['case_id'] = df_convo['case_id'].apply(lambda x: '{}_{}'.format(x.split('_')[0], x.split('_')[1].zfill(3)))\n","\n","#strip down any docket id info from case_id as well\n","df_convo['case_id'] = df_convo['case_id'].str[:8]\n","\n","df_convo['case_id'].nunique()"],"metadata":{"id":"kdtIj4UAl32e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#merge washington data with original cornell data\n","df_merged = pd.merge(df_convo, df_wash_important, on='case_id')\n","\n","#drop where there weren't matches\n","df_merged = df_merged.dropna(subset=['win_side'])\n","\n","print(df_merged.head())"],"metadata":{"id":"UcYSfXJSm4PS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#delete current case_id and replace with og_case_id so it is easier to match transcript format later\n","df_convo = df_convo.drop('case_id', axis=1)\n","\n","df_convo = df_convo.rename(columns={'og_case_id':'case_id'})"],"metadata":{"id":"mwoKXtjVoP3A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_merged.shape"],"metadata":{"id":"Ve_HbkoWm6Ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_merged.to_csv('/content/drive/MyDrive/INFO251Final/Outcomes_Dataframe_CleanedMerged.csv')"],"metadata":{"id":"3JNmLaKVn5PY"},"execution_count":null,"outputs":[]}]}